{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3dec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b33795b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data_train=pd.read_csv('train.csv')\n",
    "data_test=pd.read_csv('test.csv')\n",
    "\n",
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c22c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relleno nulos\n",
    "data_train['Age']=data_train['Age'].fillna(data_train['Age'].mean())\n",
    "#mapeo male a 0 y female a 1\n",
    "data_train['Sex'] = data_train['Sex'].map({'male': 0, 'female': 1})\n",
    "#Separamos Features (X) y Target (y)\n",
    "features=['Pclass','Sex','Age','SibSp','Parch','Fare']\n",
    "x_raw = data_train[features].values \n",
    "y_raw = data_train['Survived'].values.reshape(-1, 1)\n",
    "# NORMALIZACIÃ“N (CRÃTICO PARA REDES NEURONALES)\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2973b6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Engineering Completado ---\n",
      "Forma de X: (891, 6)\n",
      "Forma de y: (891, 1)\n",
      "Ejemplo de datos (fila 0): \n",
      "[ 0.82737724 -0.73769513 -0.5924806   0.43279337 -0.47367361 -0.50244517]\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Data Engineering Completado ---\")\n",
    "print(f\"Forma de X: {x_scaled.shape}\") # DeberÃ­a ser (891, 6)\n",
    "print(f\"Forma de y: {y_raw.shape}\")     # DeberÃ­a ser (891, 1)\n",
    "print(f\"Ejemplo de datos (fila 0): \\n{x_scaled[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2217b",
   "metadata": {},
   "source": [
    "1) Crea la clase TitanicDataset (Igual que hiciste con HouseDataset).\n",
    "\n",
    "2) InstÃ¡nciala con los nuevos datos.\n",
    "\n",
    "3) Crea el DataLoader (batch_size=32, shuffle=True).\n",
    "\n",
    "4) Define la arquitectura TitanicModel.\n",
    "\n",
    "* Entrada: Ahora son 6 features (Pclass, Sex, Age, SibSp, Parch, Fare).\n",
    "\n",
    "* Ocultas: Usa 2 capas ocultas (puedes probar con 32 neuronas cada una).\n",
    "\n",
    "* Salida: 1 neurona (binaria).\n",
    "\n",
    "* ActivaciÃ³n: ReLU en las ocultas, nada en la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a971308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, features,targets):\n",
    "        self.features=torch.tensor(data=features, dtype=torch.float32)\n",
    "        self.targets=torch.tensor(data=targets, dtype=torch.float32)\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, index):\n",
    "        # DEVOLVEMOS TUPLA: (Datos, Precio)\n",
    "        return self.features[index], self.targets[index]\n",
    "#instanciamos el dataset\n",
    "titanic_dataset=TitanicDataset(x_scaled, y_raw)\n",
    "#Crear el DataLoader\n",
    "#batch_size=32: EntregarÃ¡ paquetes de 32 datos\n",
    "#shuffle=True: BarajarÃ¡ los datos en cada Ã©poca\n",
    "loader = DataLoader(dataset=titanic_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b3f9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #primera de 3 a 32 ocultas. Entrada -> Oculta\n",
    "        self.capa1=nn.Linear(in_features=6,out_features=32)\n",
    "        self.activacion=nn.ReLU()\n",
    "        #oculta -> oculta. segunda transformaciÃ³n (Profundidad): De 32 a 32\n",
    "        self.capa2=nn.Linear(in_features=32,out_features=32)\n",
    "        #capa Final (CompresiÃ³n): De 32 a 1 solo precio\n",
    "        self.salida=nn.Linear(in_features=32,out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Debe pasar x a travÃ©s de: capa1 -> activacion -> capa2 -> activacion -> salida.\"\"\"\n",
    "        x=self.capa1(x)\n",
    "        x=self.activacion(x)\n",
    "        x=self.capa2(x)\n",
    "        x=self.activacion(x)\n",
    "        x=self.salida(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a9230ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã‰poch 1 | Loss Promedio: 0.50849 | Accuracy: 76.43%\n",
      "Ã‰poch 2 | Loss Promedio: 0.43295 | Accuracy: 80.47%\n",
      "Ã‰poch 3 | Loss Promedio: 0.42022 | Accuracy: 82.27%\n",
      "Ã‰poch 4 | Loss Promedio: 0.41042 | Accuracy: 82.27%\n",
      "Ã‰poch 5 | Loss Promedio: 0.40946 | Accuracy: 82.27%\n",
      "Ã‰poch 6 | Loss Promedio: 0.40983 | Accuracy: 82.27%\n",
      "Ã‰poch 7 | Loss Promedio: 0.40676 | Accuracy: 82.94%\n",
      "Ã‰poch 8 | Loss Promedio: 0.41284 | Accuracy: 81.93%\n",
      "Ã‰poch 9 | Loss Promedio: 0.41400 | Accuracy: 83.73%\n",
      "Ã‰poch 10 | Loss Promedio: 0.39684 | Accuracy: 82.72%\n",
      "Ã‰poch 11 | Loss Promedio: 0.40339 | Accuracy: 82.49%\n",
      "Ã‰poch 12 | Loss Promedio: 0.39398 | Accuracy: 84.06%\n",
      "Ã‰poch 13 | Loss Promedio: 0.38941 | Accuracy: 83.05%\n",
      "Ã‰poch 14 | Loss Promedio: 0.38460 | Accuracy: 84.18%\n",
      "Ã‰poch 15 | Loss Promedio: 0.38625 | Accuracy: 83.16%\n",
      "Ã‰poch 16 | Loss Promedio: 0.38180 | Accuracy: 83.28%\n",
      "Ã‰poch 17 | Loss Promedio: 0.39154 | Accuracy: 82.94%\n",
      "Ã‰poch 18 | Loss Promedio: 0.38404 | Accuracy: 82.72%\n",
      "Ã‰poch 19 | Loss Promedio: 0.38667 | Accuracy: 83.16%\n",
      "Ã‰poch 20 | Loss Promedio: 0.38647 | Accuracy: 82.94%\n",
      "Ã‰poch 21 | Loss Promedio: 0.38712 | Accuracy: 83.61%\n",
      "Ã‰poch 22 | Loss Promedio: 0.37807 | Accuracy: 83.61%\n",
      "Ã‰poch 23 | Loss Promedio: 0.37935 | Accuracy: 83.73%\n",
      "Ã‰poch 24 | Loss Promedio: 0.37666 | Accuracy: 83.61%\n",
      "Ã‰poch 25 | Loss Promedio: 0.37097 | Accuracy: 84.51%\n",
      "Ã‰poch 26 | Loss Promedio: 0.37316 | Accuracy: 83.73%\n",
      "Ã‰poch 27 | Loss Promedio: 0.37140 | Accuracy: 85.19%\n",
      "Ã‰poch 28 | Loss Promedio: 0.37314 | Accuracy: 84.40%\n",
      "Ã‰poch 29 | Loss Promedio: 0.36724 | Accuracy: 84.40%\n",
      "Ã‰poch 30 | Loss Promedio: 0.37533 | Accuracy: 84.06%\n",
      "Ã‰poch 31 | Loss Promedio: 0.37101 | Accuracy: 85.30%\n",
      "Ã‰poch 32 | Loss Promedio: 0.36901 | Accuracy: 84.96%\n",
      "Ã‰poch 33 | Loss Promedio: 0.37183 | Accuracy: 84.74%\n",
      "Ã‰poch 34 | Loss Promedio: 0.36879 | Accuracy: 84.18%\n",
      "Ã‰poch 35 | Loss Promedio: 0.36425 | Accuracy: 84.06%\n",
      "Ã‰poch 36 | Loss Promedio: 0.36493 | Accuracy: 84.96%\n",
      "Ã‰poch 37 | Loss Promedio: 0.36248 | Accuracy: 83.28%\n",
      "Ã‰poch 38 | Loss Promedio: 0.35954 | Accuracy: 85.63%\n",
      "Ã‰poch 39 | Loss Promedio: 0.36378 | Accuracy: 84.62%\n",
      "Ã‰poch 40 | Loss Promedio: 0.35510 | Accuracy: 85.30%\n",
      "Ã‰poch 41 | Loss Promedio: 0.36788 | Accuracy: 83.50%\n",
      "Ã‰poch 42 | Loss Promedio: 0.36033 | Accuracy: 84.74%\n",
      "Ã‰poch 43 | Loss Promedio: 0.36324 | Accuracy: 83.95%\n",
      "Ã‰poch 44 | Loss Promedio: 0.35833 | Accuracy: 85.07%\n",
      "Ã‰poch 45 | Loss Promedio: 0.36039 | Accuracy: 84.29%\n",
      "Ã‰poch 46 | Loss Promedio: 0.35395 | Accuracy: 85.19%\n",
      "Ã‰poch 47 | Loss Promedio: 0.35874 | Accuracy: 83.84%\n",
      "Ã‰poch 48 | Loss Promedio: 0.36536 | Accuracy: 84.62%\n",
      "Ã‰poch 49 | Loss Promedio: 0.35920 | Accuracy: 85.30%\n",
      "Ã‰poch 50 | Loss Promedio: 0.35613 | Accuracy: 83.95%\n"
     ]
    }
   ],
   "source": [
    "modelo=TitanicModel()\n",
    "#loss clasificacion binaria\n",
    "loss_function=nn.BCEWithLogitsLoss()\n",
    "#optimizador\n",
    "optimizador=torch.optim.Adam(params=modelo.parameters(),lr=0.01)\n",
    "#bucle\n",
    "epochs=50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_acumulada=0\n",
    "    correctos_acumulados = 0\n",
    "    total_muestras = 0\n",
    "\n",
    "    for batch_x, batch_y in loader:      \n",
    "        # 1. Forward\n",
    "        prediccion = modelo(batch_x)\n",
    "        # 2. Calcular Loss\n",
    "        loss = loss_function(prediccion, batch_y)\n",
    "        # 3. Limpiar gradientes previos\n",
    "        optimizador.zero_grad()\n",
    "        # 4. Backward (Calcular gradientes)\n",
    "        loss.backward()\n",
    "        # 5. Actualizar pesos\n",
    "        optimizador.step()\n",
    "        # Acumulamos el error para monitorear (opcional)\n",
    "        loss_acumulada += loss.item()\n",
    "\n",
    "        #calculo accuracy\n",
    "        #convertir logits a probabilidades\n",
    "        probs = torch.sigmoid(prediccion)\n",
    "        #Convertir a 0 o 1 (DecisiÃ³n)\n",
    "        preds = (probs > 0.5).float()\n",
    "        #Contar aciertos (True si son iguales, False si no)\n",
    "        correctos_acumulados += (preds == batch_y).sum().item()\n",
    "        total_muestras += batch_y.size(0) # Sumamos 32 (tamaÃ±o del batch)\n",
    "    # Reporte por epoch\n",
    "    promedio_loss = loss_acumulada / len(loader)\n",
    "    accuracy_epoch = correctos_acumulados / total_muestras\n",
    "    print(f\"Ã‰poch {epoch+1} | Loss Promedio: {promedio_loss:.5f} | Accuracy: {accuracy_epoch*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b99e0",
   "metadata": {},
   "source": [
    "Para cerrar este curso con broche de oro, vamos a hacer una inferencia divertida pero tÃ©cnica. Vamos a preguntarles a tu modelo quÃ© opina sobre los protagonistas de la pelÃ­cula: Jack y Rose.\n",
    "\n",
    "El problema de la Escala (Recordatorio CrÃ­tico): Tu modelo aprendiÃ³ viendo datos como \"Edad: 0.5\" (normalizado), no \"Edad: 20\". Si le pasas los datos crudos de Jack, la red pensarÃ¡ que tiene una edad gigante y fallarÃ¡. ðŸ‘‰ Regla: Debemos usar el mismo scaler que usaste en el entrenamiento para transformar a Jack y Rose.\n",
    "\n",
    "Datos de los Personajes:\n",
    "1) Jack Dawson:\n",
    "\n",
    "    * Clase: 3 (Pobre)\n",
    "\n",
    "    * Sexo: Male (0)\n",
    "\n",
    "    * Edad: 20 aÃ±os\n",
    "\n",
    "    * Hermanos/Esposa (SibSp): 0\n",
    "\n",
    "    * Padres/Hijos (Parch): 0\n",
    "\n",
    "    * Tarifa: 7.25 (Barato)\n",
    "\n",
    "2) Rose DeWitt Bukater:\n",
    "\n",
    "    * Clase: 1 (Rica)\n",
    "\n",
    "    * Sexo: Female (1)\n",
    "\n",
    "    * Edad: 17 aÃ±os\n",
    "\n",
    "    * Hermanos/Esposa (SibSp): 0\n",
    "\n",
    "    * Padres/Hijos (Parch): 1 (Su madre)\n",
    "\n",
    "    * Tarifa: 100.00 (Caro, estimado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0aa0a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack: 10.60% de probabilidad -> Muere\n",
      "Rose: 98.29% de probabilidad -> Sobrevive\n"
     ]
    }
   ],
   "source": [
    "# 1. Definimos los datos crudos [Pclass, Sex, Age, SibSp, Parch, Fare]\n",
    "pasajeros = np.array([\n",
    "    [3, 0, 20, 0, 0, 7.25],   # Jack\n",
    "    [1, 1, 17, 0, 1, 100.00]  # Rose\n",
    "])\n",
    "\n",
    "# 2. ESCALADO (Vital: Usamos el scaler que ya entrenaste)\n",
    "pasajeros_scaled = scaler.transform(pasajeros)\n",
    "\n",
    "# 3. Convertir a Tensor\n",
    "# RELLENA AQUÃ: Crea el tensor float32\n",
    "pasajeros_tensor = torch.tensor(pasajeros_scaled, dtype=torch.float32)\n",
    "\n",
    "# 4. Inferencia\n",
    "modelo.eval() # Modo evaluaciÃ³n\n",
    "with torch.no_grad():\n",
    "    # RELLENA AQUÃ: Forward pass\n",
    "    logits = modelo(pasajeros_tensor)\n",
    "    \n",
    "    # RELLENA AQUÃ: Sigmoide para obtener probabilidad\n",
    "    probs = torch.sigmoid(logits)\n",
    "\n",
    "# 5. Resultados\n",
    "nombres = [\"Jack\", \"Rose\"]\n",
    "for i, nombre in enumerate(nombres):\n",
    "    p = probs[i].item()\n",
    "    estado = \"Sobrevive\" if p > 0.5 else \"Muere\"\n",
    "    print(f\"{nombre}: {p*100:.2f}% de probabilidad -> {estado}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
