{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6910cc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de datos: 1000\n",
      "Tamaño del batch: 32\n",
      "Lotes totales por época: 32 (1000 / 32 ≈ 32 lotes)\n",
      "\n",
      "Forma del batch X: torch.Size([32, 10])\n",
      "Forma del batch Y: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# PASO 1: Crear nuestro Dataset personalizado\n",
    "class MiDatasetNumerico(Dataset):\n",
    "    def __init__(self):\n",
    "        # Simulamos 1000 muestras de datos\n",
    "        # X: 1000 vectores de 10 números cada uno\n",
    "        self.x = torch.randn(1000, 10)\n",
    "        # Y: 1000 etiquetas (0 o 1)\n",
    "        self.y = torch.randint(0, 2, (1000, 1)).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Devuelve el tamaño total\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Devuelve la muestra 'idx' y su etiqueta\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# PASO 2: Instanciar el Dataset\n",
    "mi_dataset = MiDatasetNumerico()\n",
    "\n",
    "# PASO 3: Crear el DataLoader\n",
    "# batch_size=32: Entregará paquetes de 32 datos\n",
    "# shuffle=True: Barajará los datos en cada época\n",
    "loader = DataLoader(dataset=mi_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# PASO 4: Simular una iteración de entrenamiento\n",
    "print(f\"Total de datos: {len(mi_dataset)}\")\n",
    "print(f\"Tamaño del batch: 32\")\n",
    "print(f\"Lotes totales por época: {len(loader)} (1000 / 32 ≈ 32 lotes)\")\n",
    "\n",
    "# Extraemos el primer lote para verlo\n",
    "primer_batch16 = next(iter(loader))\n",
    "datos_x16, etiquetas_y16 = primer_batch16\n",
    "\n",
    "print(f\"\\nForma del batch X: {datos_x16.shape}\") # Esperado: [32, 10]\n",
    "print(f\"Forma del batch Y: {etiquetas_y16.shape}\") # Esperado: [32, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6568d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida de la red: 0.46665969491004944\n"
     ]
    }
   ],
   "source": [
    "class RedProfunda(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.capa1=nn.Linear(in_features=10,out_features=5)\n",
    "        self.activacion=nn.ReLU()\n",
    "        self.capa2=nn.Linear(in_features=5,out_features=1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.capa1(x)\n",
    "        x=self.activacion(x)\n",
    "        x=self.capa2(x)\n",
    "        salida=x\n",
    "        return salida\n",
    "\n",
    "modelo=RedProfunda()\n",
    "input_data = torch.randn(1, 10)\n",
    "resultado16 = modelo(input_data) # OJO: No llamamos a .forward(), llamamos al objeto\n",
    "print(f\"Salida de la red: {resultado16.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2c64e5",
   "metadata": {},
   "source": [
    "Objetivo: Simular una pasada (un step) de entrenamiento completo.\n",
    "\n",
    "1) Usa la clase RedProfunda que creaste en el módulo anterior.\n",
    "\n",
    "2) Crea una instancia del DataLoader que te di en el ejemplo de arriba (con batch de 16 en lugar de 32).\n",
    "\n",
    "3) Obtén un solo batch de datos (inputs, labels) del DataLoader.\n",
    "\n",
    "4) Pasa esos inputs a través de tu modelo RedProfunda.\n",
    "\n",
    "5) Imprime la forma (shape) del tensor de salida.\n",
    "\n",
    "Pregunta trampa: Si el batch size es 16 y tu modelo saca 1 valor por dato... ¿Cuál debería ser la forma exacta (shape) de tu salida final?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e6b2e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida de la red: torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "#instancia de batch de 16\n",
    "loader16=DataLoader(dataset=mi_dataset, batch_size=16, shuffle=True)\n",
    "#un solo batch de datos\n",
    "primer_batch16 = next(iter(loader16))\n",
    "datos_x16, etiquetas_y16 = primer_batch16\n",
    "#paso a traves de red profunda\n",
    "modelo=RedProfunda()\n",
    "resultado16=modelo(datos_x16)\n",
    "print(f\"Salida de la red: {resultado16.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6ecca",
   "metadata": {},
   "source": [
    "Los Componentes del Aprendizaje\n",
    "1. Para entrenar, necesitamos dos objetos más además del Modelo y el DataLoader:\n",
    "   \n",
    "   A) La Función de Pérdida (Loss Function / Criterion):Es el juez. \n",
    "   \n",
    "   Mide qué tan mal lo hizo el modelo.\n",
    "   \n",
    "   * Si predice 0.9 y la realidad es 1.0 $\\rightarrow$ Error pequeño.\n",
    "   * Si predice 0.2 y la realidad es 1.0 $\\rightarrow$ Error grande.\n",
    "   \n",
    "   Ejemplos: nn.MSELoss (Error Cuadrático, para regresión), nn.CrossEntropyLoss (Para clasificación).\n",
    "   \n",
    "   B) El Optimizador:Es el mecánico. Toma los gradientes calculados y actualiza los pesos ($W$) para reducir el error.Ejemplos: SGD (Stochastic Gradient Descent), Adam (el más popular hoy en día).\n",
    "\n",
    "2. La Anatomía del Loop\n",
    "   \n",
    "   Un ciclo de entrenamiento en PyTorch siempre sigue estos 5 pasos sagrados dentro de un bucle:\n",
    "   * Forward Pass: El modelo hace una predicción (pred = model(x)).\n",
    "   * Calcular Loss: Comparamos predicción vs realidad (loss = criterion(pred, y)).\n",
    "   * Zero Grad: Limpiamos los gradientes viejos (optimizer.zero_grad()). PyTorch acumula gradientes por defecto, si no los limpias, se suman y arruinan el cálculo.\n",
    "   * Backward Pass: Calculamos los nuevos gradientes (loss.backward()).\n",
    "   * Optimizer Step: Actualizamos los pesos (optimizer.step()).\n",
    "\n",
    "3. Implementación CompletaVamos a entrenar tu RedProfunda para que aprenda a predecir los datos de nuestro dataset falso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3783d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Iniciando entrenamiento...\n",
      "Época 1 | Loss Promedio: 0.3214\n",
      "Época 2 | Loss Promedio: 0.2757\n",
      "Época 3 | Loss Promedio: 0.2662\n",
      "Época 4 | Loss Promedio: 0.2621\n",
      "Época 5 | Loss Promedio: 0.2587\n",
      "¡Entrenamiento finalizado!\n"
     ]
    }
   ],
   "source": [
    "# 1. Configuración\n",
    "modelo = RedProfunda() # Tu arquitectura\n",
    "criterion = nn.MSELoss() # Función de pérdida (Error Cuadrático Medio)\n",
    "optimizer = optim.SGD(modelo.parameters(), lr=0.01) # Learning Rate = 0.01\n",
    "\n",
    "# Usamos tu loader de 16\n",
    "loader16 = DataLoader(dataset=mi_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 2. El Bucle de Entrenamiento\n",
    "epochs = 5 # Cuántas veces vemos el dataset completo\n",
    "\n",
    "print(\"¡Iniciando entrenamiento...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_acumulada = 0\n",
    "    \n",
    "    for batch_x, batch_y in loader16:\n",
    "        # --- LOS 5 PASOS SAGRADOS ---\n",
    "        \n",
    "        # 1. Forward\n",
    "        prediccion = modelo(batch_x)\n",
    "        \n",
    "        # 2. Calcular Loss\n",
    "        loss = criterion(prediccion, batch_y)\n",
    "        \n",
    "        # 3. Limpiar gradientes previos\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. Backward (Calcular gradientes)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Actualizar pesos\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Acumulamos el error para monitorear (opcional)\n",
    "        loss_acumulada += loss.item()\n",
    "    \n",
    "    # Reporte por época\n",
    "    promedio_loss = loss_acumulada / len(loader16)\n",
    "    print(f\"Época {epoch+1} | Loss Promedio: {promedio_loss:.4f}\")\n",
    "\n",
    "print(\"¡Entrenamiento finalizado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a752045",
   "metadata": {},
   "source": [
    "Vas a modificar el código de arriba para usar el optimizador Adam (en lugar de SGD), que suele converger más rápido.\n",
    "\n",
    "Tu Misión:\n",
    "\n",
    "* Instancia el modelo RedProfunda.\n",
    "\n",
    "* Define la Loss Function (MSELoss).\n",
    "\n",
    "* Define el optimizador usando torch.optim.Adam con un learning rate (lr) de 0.001.\n",
    "\n",
    "* Escribe el bucle de entrenamiento (puedes copiar la estructura, pero escríbela tú mismo para generar memoria muscular) por 3 épocas.\n",
    "\n",
    "* Imprime la Loss de cada época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13ec58bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Iniciando entrenamiento...\n",
      "Época 1 | Loss Promedio: 0.2953\n",
      "Época 2 | Loss Promedio: 0.2573\n",
      "Época 3 | Loss Promedio: 0.2535\n"
     ]
    }
   ],
   "source": [
    "#configuracion\n",
    "modelo=RedProfunda()\n",
    "loss_function=nn.MSELoss()\n",
    "optimizador=optim.Adam(modelo.parameters(),lr=0.01)\n",
    "#loader de 16\n",
    "loader16 = DataLoader(dataset=mi_dataset, batch_size=16, shuffle=True)\n",
    "#epochs Cuántas veces vemos el dataset completo\n",
    "epochs=3\n",
    "print(\"¡Iniciando entrenamiento...\")\n",
    "for epoch in range(epochs):\n",
    "    loss_acum=0\n",
    "    for batch_x, batch_y in loader16:\n",
    "        # 1. Forward\n",
    "        prediccion = modelo(batch_x)\n",
    "        \n",
    "        # 2. Calcular Loss\n",
    "        loss = loss_function(prediccion, batch_y)\n",
    "        \n",
    "        # 3. Limpiar gradientes previos\n",
    "        optimizador.zero_grad()\n",
    "        \n",
    "        # 4. Backward (Calcular gradientes)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Actualizar pesos\n",
    "        optimizador.step()\n",
    "        \n",
    "        # Acumulamos el error para monitorear (opcional)\n",
    "        loss_acum += loss.item()\n",
    "    \n",
    "    # Reporte por epoch\n",
    "    promedio_loss = loss_acum / len(loader16)\n",
    "    print(f\"Época {epoch+1} | Loss Promedio: {promedio_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
